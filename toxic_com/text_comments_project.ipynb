{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Следует обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Необходимо построить модель со значением метрики качества ***F1* не меньше 0.75**. \n",
    "\n",
    "**План выполнения проекта**\n",
    "\n",
    "1. Загрузить и подготовить данные.\n",
    "2. Обучить разные модели. \n",
    "3. Сделать выводы.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\whois\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\whois\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\whois\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\whois\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# импортирование всех нужных библиотек и функций \n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "\n",
    "# загрука необходимых пакетов\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# игнорирование предупреждений \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обзор данных\n",
    "По первому взгляду на данные можно сказать:\n",
    "\n",
    "* Имеется не несущий информацию столбец 'Unnamed: 0', который скорее всего возник при сохранении и повторном чтении датафрейма, следует указать параметр `index_col` для корректного чтения.\n",
    "* Пропущенные значения отсутствуют.\n",
    "* Два возможных значения целевого признака дают понять, что перед нами задача бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# формирование датафрейма\n",
    "data = pd.read_csv('dataset_comments.csv', index_col=0)\n",
    "\n",
    "# обзор данных \n",
    "display(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распределение целевого признака\n",
    "Позитивных комментариев почти в 9 раз больше, чем негативных, это стоит учесть при разделении данных на выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# количество объектов на каждое значение целевого признака\n",
    "display(data['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очистка текста\n",
    "\n",
    "* Удалим лишние символы.\n",
    "* Заменим ссылки и числа на 'URL' и 'NUM' соответственно.\n",
    "* Удалим лишние пробелы.\n",
    "* Очистим текст от стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список с английскими стоп-словами\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# функция по очистке текста\n",
    "def clean_text(text):\n",
    "    # привидение текста к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # удаление лишних символов \n",
    "    text = re.sub(r'[\\*+\\#+\\№\\\"\\-+\\+\\=+\\?+\\&\\^\\.+\\;\\,+\\>+\\(\\)\\/+\\:\\\\+]', '', text)\n",
    "    # замена ссылок на 'URL'\n",
    "    text = re.sub(r'(http\\S+)|(www\\S+)|([\\w\\d]+www\\S+)|([\\w\\d]+http\\S+)', r'URL', text)\n",
    "    # замена чисел на 'NUM'\n",
    "    text = re.sub(r'(\\d+\\s\\d+)|(\\d+)',' NUM ', text)\n",
    "    # удаление лишних пробелов\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # очистка текста от стоп-слов\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание нового столбца с очищенным текстом\n",
    "data['lemm_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация текста "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# перевод объекта Series в список \n",
    "corpus = data['lemm_text'].tolist()\n",
    "\n",
    "# пустой список для лемматизированного текста\n",
    "lemm_corpus = []\n",
    "\n",
    "# лемматизатор\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# цикл лемматизации текста \n",
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].split()\n",
    "    lemm_list = []\n",
    "    for j in range(len(corpus[i])):\n",
    "        lemm_list.append(wnl.lemmatize(corpus[i][j]))\n",
    "    lemm_list = ' '.join(lemm_list)\n",
    "    lemm_corpus.append(lemm_list)\n",
    "\n",
    "# добавление столбца с лемматизированным текстом \n",
    "data['lemm_text'] = lemm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение данных\n",
    "\n",
    "Разделение данных на обучающую, валидационную и тестовую выборки в соотношении 3:1:1 соответственно, с учетом дисбаланса классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "31914"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "31915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# формирование обучающей, валидационной и тестовой выборок\n",
    "data_train, data_temporary = train_test_split(data, test_size=0.4, random_state=42, stratify=data['toxic'])\n",
    "data_valid, data_test = train_test_split(data_temporary, test_size=0.5, random_state=42, stratify=data_temporary['toxic'])\n",
    "\n",
    "# проверка соотношения размеров выборок\n",
    "display(data_train.shape[0], data_valid.shape[0], data_test.shape[0])\n",
    "\n",
    "# счетчик величин TI-IDF с учетом стоп-слов\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# признаки и целевой признак в обучающей выборке \n",
    "features_train = count_tf_idf.fit_transform(data_train['lemm_text'])\n",
    "target_train = data_train['toxic']\n",
    "\n",
    "# признаки и целевой признак в валидационной выборке \n",
    "features_valid = count_tf_idf.transform(data_valid['lemm_text'])\n",
    "target_valid = data_valid['toxic']\n",
    "\n",
    "# признаки и целевой признак в тестовой выборке \n",
    "features_test = count_tf_idf.transform(data_test['lemm_text'])\n",
    "target_test = data_test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "Рассмотрим три модели: \n",
    "* Логистическая регрессия.\n",
    "* RandomForestClassifier.\n",
    "* CatBoostClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция нахождения наилучшего порога вероятности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для нахождения наилучшего порога, возвращает порог, f1-меру с заданным порогом и матрицу ошибок\n",
    "def best_threshold(probabilities_one, target):\n",
    "    max_f1_score = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.arange(0, 0.8, 0.02):\n",
    "        predicted = probabilities_one > threshold\n",
    "        f1 = f1_score(predicted, target)\n",
    "        if max_f1_score < f1:\n",
    "            max_f1_score = f1\n",
    "            best_threshold = threshold\n",
    "            matrix = confusion_matrix(target, predicted)\n",
    "    return (f\"Значение F1-меры = {max_f1_score:.3f}, при пороге вероятности: {best_threshold}\", matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия\n",
    "Показатели модели без подбора порога:\n",
    "* **F1-мера: 0.739.**\n",
    "\n",
    "Показатели модели с подобранным порогом:\n",
    "* **F1-мера: 0.781.**\n",
    "* Порог вероятности: 0.72.\n",
    "\n",
    "Стоит отметить, что модель с порогом вероятности 0.72 показывает F1-меру выше, нежели модель со стандартным значением порога, но при этом она дает **меньше ложно-положительных ответов и больше ложно-отрицательных ответов**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# вариации гиперпараметров\n",
    "fit_intercept = [True, False]\n",
    "class_weight = ['balanced']\n",
    "\n",
    "# сетка гиперпараметров\n",
    "param_grid = {'fit_intercept':fit_intercept,\n",
    "              'class_weight':class_weight}\n",
    "\n",
    "# модель 'логистическая регрессия'\n",
    "model_log_reg = GridSearchCV(LogisticRegression(random_state=42), param_grid=param_grid, scoring='f1', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Значение F1-меры = 0.739'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[27238,  1431],\n",
       "       [  507,  2738]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.3 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# обучение модели\n",
    "model_log_reg.fit(features_train, target_train)\n",
    "\n",
    "# предсказания модели\n",
    "pred_valid = model_log_reg.predict(features_valid)\n",
    "\n",
    "# значение F1-меры и матрица ошибок без подбора порога \n",
    "display(f\"Значение F1-меры = {f1_score(target_valid, pred_valid):.3f}\")\n",
    "display(confusion_matrix(target_valid, pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Значение F1-меры = 0.781, при пороге вероятности: 0.72',\n",
       " array([[28161,   508],\n",
       "        [  839,  2406]], dtype=int64))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# вероятность положительного значения целевого признака\n",
    "probabilities_one_valid = model_log_reg.predict_proba(features_valid)[:, 1]\n",
    "\n",
    "# максимальное значение F1-меры с учетом нахождения порога вероятности\n",
    "display(best_threshold(probabilities_one_valid, target_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier\n",
    "Показатели модели без подбора порога:\n",
    "* **F1-мера: 0.495.**\n",
    "\n",
    "Показатели модели с подобранным порогом:\n",
    "* **F1-мера: 0.659.**\n",
    "* Порог вероятности: 0.54.\n",
    "\n",
    "Модель 'RandomForestClassifier' с подобранным порогом вероятности показывает F1-меру выше, нежели модель со стандартным значением порога, но при этом она дает **меньше ложно-положительных ответов и больше ложно-отрицательных ответов**, как и модель 'логистическая регрессия'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариации гиперпараметров\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 3)]\n",
    "n_estimators = [int(x) for x in np.linspace(10, 100, num = 3)]\n",
    "class_weight = ['balanced']\n",
    "\n",
    "# сетка гиперпараметров\n",
    "param_grid = {'max_depth':max_depth,\n",
    "              'n_estimators':n_estimators,\n",
    "              'class_weight':class_weight}\n",
    "\n",
    "# модель 'случайный лес'\n",
    "model_forest = GridSearchCV(RandomForestClassifier(random_state=42), param_grid=param_grid, scoring='f1', cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Значение F1-меры = 0.495'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[24280,  4389],\n",
       "       [  736,  2509]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 5s\n",
      "Wall time: 5min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# обучение модели\n",
    "model_forest.fit(features_train, target_train)\n",
    "\n",
    "# предсказания модели на валидационной выборке\n",
    "pred_valid = model_forest.predict(features_valid)\n",
    "\n",
    "# значение F1-меры и матрица ошибок без подбора порога \n",
    "display(f\"Значение F1-меры = {f1_score(target_valid, pred_valid):.3f}\")\n",
    "display(confusion_matrix(target_valid, pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Значение F1-меры = 0.659, при пороге вероятности: 0.54',\n",
       " array([[27866,   803],\n",
       "        [ 1256,  1989]], dtype=int64))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.25 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# вероятность положительного значения целевого признака\n",
    "probabilities_one_valid = model_forest.predict_proba(features_valid)[:, 1]\n",
    "\n",
    "# максимальное значение F1-меры с учетом нахождения порога вероятности\n",
    "display(best_threshold(probabilities_one_valid, target_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostClassifier\n",
    "Показатели модели без подбора порога:\n",
    "* **F1-мера: 0.752.**\n",
    "\n",
    "Показатели модели с подобранным порогом:\n",
    "* **F1-мера: 0.777.**\n",
    "* Порог вероятности: 0.66.\n",
    "\n",
    "Аналогичный результат и у модели 'CatBoostClassifier': с подобранным порогом вероятности показывает F1-меру выше, нежели модель со стандартным значением порога, но при этом она дает **меньше ложно-положительных ответов и больше ложно-отрицательных ответов**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.072254\n",
      "0:\tlearn: 0.5128754\ttotal: 590ms\tremaining: 9m 49s\n",
      "100:\tlearn: 0.8264020\ttotal: 50.4s\tremaining: 7m 28s\n",
      "200:\tlearn: 0.8643951\ttotal: 1m 39s\tremaining: 6m 33s\n",
      "300:\tlearn: 0.8837338\ttotal: 2m 27s\tremaining: 5m 43s\n",
      "400:\tlearn: 0.8987199\ttotal: 3m 16s\tremaining: 4m 52s\n",
      "500:\tlearn: 0.9089234\ttotal: 4m 4s\tremaining: 4m 3s\n",
      "600:\tlearn: 0.9185545\ttotal: 4m 52s\tremaining: 3m 14s\n",
      "700:\tlearn: 0.9248350\ttotal: 5m 40s\tremaining: 2m 25s\n",
      "800:\tlearn: 0.9320198\ttotal: 6m 28s\tremaining: 1m 36s\n",
      "900:\tlearn: 0.9373197\ttotal: 7m 16s\tremaining: 47.9s\n",
      "999:\tlearn: 0.9419959\ttotal: 8m 3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Значение F1-меры = 0.752'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[27471,  1198],\n",
       "       [  567,  2678]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 1min 51s\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# модель 'CatBoostClassifier'\n",
    "model_cat = CatBoostClassifier(random_state=42, iterations=1000, auto_class_weights='Balanced', \n",
    "                               eval_metric='F1', verbose=100, loss_function='Logloss')\n",
    "# обучение модели\n",
    "model_cat.fit(features_train, target_train)\n",
    "\n",
    "# предсказания модели на валидационной выборке\n",
    "pred_valid = model_cat.predict(features_valid)\n",
    "\n",
    "# значение F1-меры и матрица ошибок без подбора порога \n",
    "display(f\"Значение F1-меры = {f1_score(target_valid, pred_valid):.3f}\")\n",
    "display(confusion_matrix(target_valid, pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Значение F1-меры = 0.777, при пороге вероятности: 0.66',\n",
       " array([[28080,   589],\n",
       "        [  807,  2438]], dtype=int64))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.95 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# вероятность положительного значения целевого признака\n",
    "probabilities_one_valid = model_cat.predict_proba(features_valid)[:, 1]\n",
    "\n",
    "# максимальное значение F1-меры с учетом нахождения порога вероятности\n",
    "display(best_threshold(probabilities_one_valid, target_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение моделей\n",
    "По совокупным результам наилучшие показатели имеет модель 'логистическая регрессия' с подобранным порогом вероятности. <br>\n",
    "**F1-мера модели на валидационных данных: 0.781.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_score</th>\n",
       "      <th>F1_score_with_threshold</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.739</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.495</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        F1_score  F1_score_with_threshold  threshold\n",
       "LogisticRegression         0.739                    0.781       0.72\n",
       "RandomForestClassifier     0.495                    0.659       0.54\n",
       "CatBoostClassifier         0.752                    0.777       0.66"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# датафрейм с результатами моделей \n",
    "models_stats = pd.DataFrame(data={'F1_score':[.739, .495, .752], \n",
    "                                  'F1_score_with_threshold':[.781, .659, .777],\n",
    "                                  'threshold':[.72, .54, .66]}, \n",
    "                            index=['LogisticRegression', 'RandomForestClassifier', 'CatBoostClassifier'])\n",
    "display(models_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с константной моделью\n",
    "\n",
    "Для проверки модели на адекватность сравним ее с результами константной модели. <br>\n",
    "**F1-мера константной модели: 0.171**, выбранная модель показывает результаты лучше, следовательно модель адекватна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17125605122065482"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# константная модель \n",
    "model_dummy = DummyClassifier(strategy='uniform', random_state=42)\n",
    "\n",
    "# обучение константной модели\n",
    "model_dummy.fit(features_train, target_train)\n",
    "\n",
    "# F1-мера константной модели\n",
    "display(f1_score(target_valid, model_dummy.predict(features_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7737630208333334"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# предсказания выбранной модели на тестовой выборке\n",
    "pred_test = model_log_reg.predict(features_test)\n",
    "\n",
    "# вероятность положительного значения целевого признака\n",
    "proba_one_test = model_log_reg.predict_proba(features_test)[:, 1]\n",
    "\n",
    "# установка порога\n",
    "pred_test = proba_one_test > .72\n",
    "\n",
    "# F1-мера выбранной модели на тестовой выборке\n",
    "display(f1_score(target_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Провели первичный обзор данных.\n",
    "* Проанализировали распределение целевого признака.\n",
    "* Очистили текст от лишних символов и пробелов, заменили ссылки и числа на 'URL' и 'NUM' соответственно, очистили текст от стоп-слов.\n",
    "* Лемматизировали текст.\n",
    "* Разделили данные на обучающую, валидационную и тестовую выборки в соотношении 3:1:1 соответственно, учли при этом дисбаланс классов.\n",
    "* Обучили и сравнили между собой три разные модели: 'логистическая регрессия', 'RandomForestClassifier', 'CatBoostClassifier'. Подобрали наилучшую модель по F1-мере с подобранным порогом вероятности классов - 'логистическая регрессия'.\n",
    "* Провели проверку выбранной модели на адекватность, путем сравнения ее с показателями константной модели.\n",
    "* Протестировали выбранную модель на тестовой выборке. **Конечная F1-мера модели на тестовых данных: 0.773**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
